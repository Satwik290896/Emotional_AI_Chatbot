{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmAgpp3Iw7iW",
        "outputId": "b837d3be-24dc-4379-88ee-250c118f5ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0rf-HUj93Z9D",
        "outputId": "7679bc40-1e25-4408-e482-b030a2d4b336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "Sun May  8 17:26:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cd content/drive/MyDrive/cifar10_resnet"
      ],
      "metadata": {
        "id": "3wHVImLw9pcD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "!pip install emoji\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r', encoding=\"UTF-8\") as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "def read_csv(filename = 'emojify_data.csv'):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "\n",
        "\n",
        "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":smile:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "    \"\"\"\n",
        "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    \"\"\"\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
        "              \n",
        "    \n",
        "def print_predictions(X, pred):\n",
        "    print()\n",
        "    for i in range(X.shape[0]):\n",
        "        print(X[i], label_to_emoji(int(pred[i])))\n",
        "        \n",
        "        \n",
        "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    \n",
        "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "    \n",
        "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
        "    \n",
        "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_confusion.columns))\n",
        "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_confusion.index)\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)\n",
        "    \n",
        "    \n",
        "def predict(X, Y, W, b, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data containing sentences, numpy array of shape (m, None)\n",
        "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
        "    \n",
        "    Returns:\n",
        "    pred -- numpy array of shape (m, 1) with your predictions\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    pred = np.zeros((m, 1))\n",
        "    \n",
        "    for j in range(m):                       # Loop over training examples\n",
        "        \n",
        "        # Split jth test example (sentence) into list of lower case words\n",
        "        words = X[j].lower().split()\n",
        "        \n",
        "        # Average words' vectors\n",
        "        avg = np.zeros((50,))\n",
        "        for w in words:\n",
        "            avg += word_to_vec_map[w]\n",
        "        avg = avg/len(words)\n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(W, avg) + b\n",
        "        A = softmax(Z)\n",
        "        pred[j] = np.argmax(A)\n",
        "        \n",
        "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
        "    \n",
        "    return pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIMkIkQzaWdN",
        "outputId": "1746fb9f-e3bb-4d0c-ca5f-130f94e0a51b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 35.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 14.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=a26f1cee121c0ee1e94e38d341515df0db9386d378cfc8b20616446fe973f7a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def createDataFiles(allDataFile,trainFile,testFile):  # Want to split into train and test, and put in separate (new) files\n",
        "  test_indices = []\n",
        "  for i in range(5):\n",
        "    # print(min(range(40*i,40*(i+1))),max(range(40*i,40*(i+1))))\n",
        "    test_indices += random.sample(range(40*i,40*(i+1)),6)\n",
        "  \n",
        "  with open(allDataFile,\"r\") as fileToRead:\n",
        "    csvReader = csv.reader(fileToRead)\n",
        "    i = 0\n",
        "    mode1,mode2 = \"w\",\"w\"\n",
        "    for row in csvReader:\n",
        "      if i in test_indices:\n",
        "        file = testFile\n",
        "        with open(file,mode1) as fileToWrite:\n",
        "          csvWriter = csv.writer(fileToWrite)\n",
        "          # print(row)\n",
        "          csvWriter.writerow(row)\n",
        "          mode1 = \"a\"\n",
        "      else:\n",
        "        file = trainFile\n",
        "        with open(file,mode2) as fileToWrite:\n",
        "          csvWriter = csv.writer(fileToWrite)\n",
        "          # print(row)\n",
        "          csvWriter.writerow(row)\n",
        "          mode2 = \"a\"\n",
        "\n",
        "      i+=1\n",
        "\n",
        "allDataFile,trainFile,testFile = \"/content/drive/MyDrive/Emotional_AI_Chatbot/TrainDataTrim.csv\",\\\n",
        "            \"/content/drive/MyDrive/Emotional_AI_Chatbot/RealTrainData.csv\", \"/content/drive/MyDrive/Emotional_AI_Chatbot/RealTestData.csv\"\n",
        "\n",
        "# createDataFiles(allDataFile,trainFile,testFile)"
      ],
      "metadata": {
        "id": "8CEMVVxQ-SLL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_indices = []\n",
        "# for i in range(5):\n",
        "#   # print(min(range(40*i,40*(i+1))),max(range(40*i,40*(i+1))))\n",
        "#   test_indices += random.sample(range(40*i,40*(i+1)),6)\n",
        "# print(len(test_indices))\n",
        "# print(test_indices)"
      ],
      "metadata": {
        "id": "Nz0iqK8F-33X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "import time\n",
        "import json\n",
        "# from /content/drive/MyDrive/Emotional_AI_Chatbot/emo_utils.py import *\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()`\n",
        "\n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this.\n",
        "\n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):  # loop over training examples\n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = (X[i].lower()).split()\n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            # Increment j to j + 1\n",
        "            j = j + 1\n",
        "    return X_indices\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "\n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    vocab_len = len(word_to_index) + 1  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "\n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n",
        "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
        "\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "\n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "\n",
        "    return embedding_layer\n",
        "\n",
        "\n",
        "def SentimentAnalysis(input_shape, word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Function creating the Emojify-v2 model's graph.\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the input, usually (max_len,)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    model -- a model instance in Keras\n",
        "    \"\"\"\n",
        "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
        "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
        "\n",
        "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "\n",
        "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "    embeddings = embedding_layer(sentence_indices)\n",
        "\n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a batch of sequences.\n",
        "    X = LSTM(128, return_sequences=True)(embeddings)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = LSTM(128)(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
        "    X = Dense(5, activation='softmax')(X)\n",
        "    # Add a softmax activation\n",
        "    X = Activation('softmax')(X)\n",
        "\n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(sentence_indices, X)\n",
        "    return model"
      ],
      "metadata": {
        "id": "CMw44X67K5A8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read train and test files\n",
        "    X_train, Y_train = read_csv('/content/drive/MyDrive/Emotional_AI_Chatbot/RealTrainData.csv')\n",
        "    X_test, Y_test = read_csv('/content/drive/MyDrive/Emotional_AI_Chatbot/RealTestData.csv')\n",
        "    maxLen = len(max(X_train, key=len).split())\n",
        "\n",
        "    # Convert one-hot-encoding type, classification =5, [1,0,0,0,0]\n",
        "    Y_oh_train = convert_to_one_hot(Y_train, C=5)\n",
        "    Y_oh_test = convert_to_one_hot(Y_test, C=5)\n",
        "\n",
        "    # Read 50 feature dimension glove file\n",
        "    word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/MyDrive/Emotional_AI_Chatbot/glove.6B.50d.txt')\n",
        "\n",
        "    # outputFileBackup = '/content/drive/MyDrive/Emotional_AI_Chatbot/hyp_search.txt'\n",
        "    outputFile = '/content/drive/MyDrive/Emotional_AI_Chatbot/hyp_search3.csv'\n",
        "    with open(outputFile,\"w\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      writer.writerow([\"Loss Function\",\"Batch Size\",\"Optimizer\",\"Time to Train\",\"Loss\",\"Test Accuracy\"])\n",
        "\n",
        "    j = 2 # For enumerating saved plots to match their rows in csv file\n",
        "    for lossFxn in [\"categorical_crossentropy\"]:\n",
        "      for optimizer in [\"adam\",\"RMSprop\",\"Adadelta\",\"Nadam\"]:\n",
        "        for batch_size in [8,16,32,64]:\n",
        "          # Model and model summmary\n",
        "          model = SentimentAnalysis((maxLen,), word_to_vec_map, word_to_index)\n",
        "          model.summary()\n",
        "\n",
        "          model.compile(loss=lossFxn, optimizer=optimizer, metrics=['accuracy']) # R\n",
        "          # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "          X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "          Y_train_oh = convert_to_one_hot(Y_train, C=5)\n",
        "\n",
        "          # Train model\n",
        "          t0 = time.time()\n",
        "          history = model.fit(X_train_indices, Y_train_oh, epochs=300, batch_size=batch_size,validation_split=0.1, shuffle=True)\n",
        "          t1 = time.time() # Change epochs!\n",
        "          timeToTrain = t1-t0\n",
        "          print(f\"history: {history.history}\")\n",
        "\n",
        "          X_test_indices = sentences_to_indices(X_test, word_to_index, max_len=maxLen)\n",
        "          Y_test_oh = convert_to_one_hot(Y_test, C=5)\n",
        "\n",
        "          # Evaluate model, loss and accuracy\n",
        "          loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "          print(\"Test accuracy = \", acc)\n",
        "\n",
        "          with open(outputFile,\"a\",newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([lossFxn,batch_size,optimizer,timeToTrain,loss,acc])\n",
        "\n",
        "          # writer.writerow([lossFxn,batch_size,optimizer,timeToTrain,loss,acc])\n",
        "          # print(\"Written.\")\n",
        "\n",
        "          # with open(outputFileBackup,\"w\") as backupFile:\n",
        "          #   backupFile.write(json.dumps([lossFxn,batch_size,optimizer,timeToTrain,loss,acc]))\n",
        "\n",
        "          # Compare prediction and expected emoji\n",
        "          C = 5\n",
        "          y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "          X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "          pred = model.predict(X_test_indices)\n",
        "          for i in range(len(X_test)):\n",
        "              x = X_test_indices\n",
        "              num = np.argmax(pred[i])\n",
        "              if (num != Y_test[i]):\n",
        "                  print('Expected emoji:' + label_to_emoji(Y_test[i]) + ' prediction: ' + X_test[i] + label_to_emoji(num).strip())\n",
        "\n",
        "          # Test your sentence\n",
        "          x_test = np.array(['I am very happy'])\n",
        "          X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "          print(x_test[0] + ' ' + label_to_emoji(np.argmax(model.predict(X_test_indices))))\n",
        "          \n",
        "          # Plotting results\n",
        "          # print(history.history.keys())\n",
        "          fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "          # axs[0].plot(range(1,len(history.history['accuracy'])+1),history.history['accuracy'])\n",
        "          # axs[0].set_title(f'Acc - {lossFxn[:3]},{batch_size},{optimizer}')\n",
        "          # axs[1].plot(range(1,len(history.history['loss'])+1),history.history['loss'],'tab:red')\n",
        "          # axs[1].set_title(f'Loss - {lossFxn[:3]},{batch_size},{optimizer}')\n",
        "          \n",
        "          axs[0, 0].plot(range(1,len(history.history['accuracy'])+1),history.history['accuracy'])\n",
        "          axs[0, 0].set_title(f'Train Acc - {batch_size},{optimizer}')\n",
        "          axs[1, 0].plot(range(1,len(history.history['loss'])+1),history.history['loss'],'tab:red')\n",
        "          axs[1, 0].set_title(f'Train Loss - {batch_size},{optimizer}')\n",
        "          axs[0, 1].plot(range(1,len(history.history['val_accuracy'])+1),history.history['val_accuracy'])\n",
        "          axs[0, 1].set_title(f'Val Acc - {batch_size},{optimizer}')\n",
        "          axs[1, 1].plot(range(1,len(history.history['val_loss'])+1),history.history['val_loss'],'tab:red')\n",
        "          axs[1, 1].set_title(f'Val Loss - {batch_size},{optimizer}')\n",
        "          \n",
        "          '''axs[0, 0].plot(history.history['accuracy'])\n",
        "          axs[0, 0].set_title('Train dataset Accuracy')\n",
        "          axs[0, 1].plot(history.history['loss'])\n",
        "          axs[0, 1].set_title('Training Loss')\n",
        "          axs[1, 0].plot(history.history['val_accuracy'])\n",
        "          axs[1, 0].set_title('Validation dataset Accuracy')\n",
        "          axs[1, 1].plot(history.history['val_loss'])\n",
        "          axs[1, 1].set_title('Validation Loss')'''\n",
        "          for ax in axs.flat:\n",
        "              ax.set(xlabel='Epochs', ylabel='Parameter')\n",
        "\n",
        "          # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "          # '''for ax in axs.flat:\n",
        "          #     ax.label_outer()'''\n",
        "          plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.3,hspace=0.3)\n",
        "          plt.savefig(f\"/content/drive/MyDrive/Emotional_AI_Chatbot/plots3/Row{j}.png\")\n",
        "          j+=1\n",
        "          plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "WA527AJwtWEZ",
        "outputId": "30d67316-6c15-49a8-9c0c-87b5674b83b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6008378f54b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0;31m# Model and model summmary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxLen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SentimentAnalysis' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing writing to files\n",
        "# import time\n",
        "# import json\n",
        "# import csv\n",
        "# t0 = time.time()\n",
        "# thresholdTime = 0\n",
        "# outputFile = '/content/drive/MyDrive/Emotional_AI_Chatbot/write_test.csv'\n",
        "# with open(outputFile,\"w\") as file:\n",
        "#   writer = csv.writer(file)\n",
        "#   writer.writerow([\"Loss Function\",\"Batch Size\",\"Optimizer\",\"Test Accuracy\"])\n",
        "#   t1 = time.time()\n",
        "#   while t1-t0<12:\n",
        "#     t1 = time.time()\n",
        "#     if (t1-t0)>thresholdTime:\n",
        "#       writer.writerow([\"CCE\",32,\"Adam\",0.925])\n",
        "#       # stringData = json.dumps(data)\n",
        "#       # print(stringData)\n",
        "#       # file.write(stringData)\n",
        "#       thresholdTime += 1"
      ],
      "metadata": {
        "id": "zogWL3M8-6HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test best model.\n",
        "\n",
        "model = SentimentAnalysis((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"Nadam\", metrics=['accuracy']) # R\n",
        "\n",
        "X_train, Y_train = read_csv('/content/drive/MyDrive/Emotional_AI_Chatbot/RealTrainData.csv')\n",
        "X_test, Y_test = read_csv('/content/drive/MyDrive/Emotional_AI_Chatbot/RealTestData.csv')\n",
        "maxLen = len(max(X_train, key=len).split())\n",
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C=5)\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len=maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C=5)\n",
        "\n",
        "\n",
        "# Train model\n",
        "t0 = time.time()\n",
        "history = model.fit(X_train_indices, Y_train_oh, epochs=300, batch_size=64,validation_split=0.1, shuffle=True)\n",
        "t1 = time.time() # Change epochs!\n",
        "timeToTrain = t1-t0\n",
        "print(f\"timeToTrain = {timeToTrain} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybJJ-rIwxrW",
        "outputId": "9e8d0e7a-bc30-41c4-8655-b1e078e96e82"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 22)]              0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, 22, 50)            20000050  \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 22, 128)           91648     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 22, 128)           0         \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,223,927\n",
            "Trainable params: 20,223,927\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "3/3 [==============================] - 4s 360ms/step - loss: 1.6035 - accuracy: 0.2222 - val_loss: 1.6679 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5862 - accuracy: 0.2222 - val_loss: 1.7077 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.5563 - accuracy: 0.3203 - val_loss: 1.8605 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5549 - accuracy: 0.3072 - val_loss: 1.7721 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4961 - accuracy: 0.3856 - val_loss: 1.8146 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4916 - accuracy: 0.3791 - val_loss: 1.8348 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4653 - accuracy: 0.4248 - val_loss: 1.8172 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4573 - accuracy: 0.4575 - val_loss: 1.8501 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4048 - accuracy: 0.5163 - val_loss: 1.8608 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3476 - accuracy: 0.5621 - val_loss: 1.8588 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2989 - accuracy: 0.6078 - val_loss: 1.8849 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3922 - accuracy: 0.5098 - val_loss: 1.8482 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3044 - accuracy: 0.6078 - val_loss: 1.8813 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.3749 - accuracy: 0.5163 - val_loss: 1.8831 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2388 - accuracy: 0.6667 - val_loss: 1.8660 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3020 - accuracy: 0.6078 - val_loss: 1.8867 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2078 - accuracy: 0.7190 - val_loss: 1.8852 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1884 - accuracy: 0.7320 - val_loss: 1.8909 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2151 - accuracy: 0.6928 - val_loss: 1.8921 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2366 - accuracy: 0.6667 - val_loss: 1.8865 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2485 - accuracy: 0.6667 - val_loss: 1.8939 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2763 - accuracy: 0.6144 - val_loss: 1.8955 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2240 - accuracy: 0.6797 - val_loss: 1.8689 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2288 - accuracy: 0.6732 - val_loss: 1.8613 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1937 - accuracy: 0.7059 - val_loss: 1.8839 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1594 - accuracy: 0.7582 - val_loss: 1.8907 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1208 - accuracy: 0.7843 - val_loss: 1.8743 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1306 - accuracy: 0.7843 - val_loss: 1.8858 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1488 - accuracy: 0.7582 - val_loss: 1.8746 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0998 - accuracy: 0.8039 - val_loss: 1.8799 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0864 - accuracy: 0.8105 - val_loss: 1.8759 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1079 - accuracy: 0.8039 - val_loss: 1.8919 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1844 - accuracy: 0.7124 - val_loss: 1.8823 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2193 - accuracy: 0.6928 - val_loss: 1.8699 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1810 - accuracy: 0.7320 - val_loss: 1.8383 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1892 - accuracy: 0.7190 - val_loss: 1.8154 - val_accuracy: 0.0588\n",
            "Epoch 37/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1844 - accuracy: 0.7190 - val_loss: 1.8429 - val_accuracy: 0.0588\n",
            "Epoch 38/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1726 - accuracy: 0.7255 - val_loss: 1.8897 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1485 - accuracy: 0.7712 - val_loss: 1.9004 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0819 - accuracy: 0.8366 - val_loss: 1.8514 - val_accuracy: 0.0588\n",
            "Epoch 41/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1330 - accuracy: 0.7712 - val_loss: 1.8602 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1091 - accuracy: 0.8039 - val_loss: 1.8477 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1122 - accuracy: 0.7974 - val_loss: 1.7066 - val_accuracy: 0.2353\n",
            "Epoch 44/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3311 - accuracy: 0.5817 - val_loss: 1.8609 - val_accuracy: 0.0588\n",
            "Epoch 45/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4360 - accuracy: 0.4706 - val_loss: 1.8334 - val_accuracy: 0.0588\n",
            "Epoch 46/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4426 - accuracy: 0.4641 - val_loss: 1.7018 - val_accuracy: 0.1765\n",
            "Epoch 47/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4321 - accuracy: 0.4706 - val_loss: 1.7801 - val_accuracy: 0.1176\n",
            "Epoch 48/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3615 - accuracy: 0.5425 - val_loss: 1.7396 - val_accuracy: 0.1176\n",
            "Epoch 49/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2611 - accuracy: 0.6471 - val_loss: 1.7514 - val_accuracy: 0.1765\n",
            "Epoch 50/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2458 - accuracy: 0.6667 - val_loss: 1.8579 - val_accuracy: 0.0588\n",
            "Epoch 51/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.4035 - accuracy: 0.4967 - val_loss: 1.5934 - val_accuracy: 0.2941\n",
            "Epoch 52/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2748 - accuracy: 0.6340 - val_loss: 1.7021 - val_accuracy: 0.2353\n",
            "Epoch 53/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2344 - accuracy: 0.6797 - val_loss: 1.7228 - val_accuracy: 0.1765\n",
            "Epoch 54/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2383 - accuracy: 0.6667 - val_loss: 1.7862 - val_accuracy: 0.1176\n",
            "Epoch 55/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.3103 - accuracy: 0.5948 - val_loss: 1.7963 - val_accuracy: 0.1176\n",
            "Epoch 56/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2401 - accuracy: 0.6732 - val_loss: 1.8988 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1558 - accuracy: 0.7516 - val_loss: 1.8875 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1283 - accuracy: 0.7778 - val_loss: 1.9027 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1544 - accuracy: 0.7451 - val_loss: 1.9040 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2184 - accuracy: 0.6928 - val_loss: 1.8387 - val_accuracy: 0.0588\n",
            "Epoch 61/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2466 - accuracy: 0.6601 - val_loss: 1.8452 - val_accuracy: 0.0588\n",
            "Epoch 62/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2065 - accuracy: 0.6993 - val_loss: 1.6743 - val_accuracy: 0.2353\n",
            "Epoch 63/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1052 - accuracy: 0.7974 - val_loss: 1.7629 - val_accuracy: 0.1176\n",
            "Epoch 64/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0976 - accuracy: 0.8105 - val_loss: 1.7762 - val_accuracy: 0.1176\n",
            "Epoch 65/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0629 - accuracy: 0.8431 - val_loss: 1.8527 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0512 - accuracy: 0.8562 - val_loss: 1.9037 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0502 - accuracy: 0.8562 - val_loss: 1.8448 - val_accuracy: 0.0588\n",
            "Epoch 68/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0642 - accuracy: 0.8366 - val_loss: 1.8451 - val_accuracy: 0.0588\n",
            "Epoch 69/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0960 - accuracy: 0.8105 - val_loss: 1.8959 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1340 - accuracy: 0.7712 - val_loss: 1.8959 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0440 - accuracy: 0.8627 - val_loss: 1.8908 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.0307 - accuracy: 0.8758 - val_loss: 1.8950 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0364 - accuracy: 0.8758 - val_loss: 1.8728 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1013 - accuracy: 0.7974 - val_loss: 1.7048 - val_accuracy: 0.1765\n",
            "Epoch 75/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0516 - accuracy: 0.8562 - val_loss: 1.9035 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.0470 - accuracy: 0.8562 - val_loss: 1.7880 - val_accuracy: 0.1176\n",
            "Epoch 77/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0271 - accuracy: 0.8758 - val_loss: 1.9040 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0027 - accuracy: 0.9085 - val_loss: 1.9037 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9924 - accuracy: 0.9150 - val_loss: 1.9044 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.0455 - accuracy: 0.8562 - val_loss: 1.8622 - val_accuracy: 0.0588\n",
            "Epoch 81/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.0122 - accuracy: 0.8954 - val_loss: 1.9005 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.3455 - accuracy: 0.5556 - val_loss: 1.9028 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2638 - accuracy: 0.6405 - val_loss: 1.9019 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.2462 - accuracy: 0.6667 - val_loss: 1.9034 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2256 - accuracy: 0.6797 - val_loss: 1.9021 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1954 - accuracy: 0.7124 - val_loss: 1.8334 - val_accuracy: 0.0588\n",
            "Epoch 87/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2365 - accuracy: 0.6732 - val_loss: 1.9029 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2190 - accuracy: 0.6863 - val_loss: 1.9033 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/300\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.2192 - accuracy: 0.6863 - val_loss: 1.8581 - val_accuracy: 0.0588\n",
            "Epoch 90/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1807 - accuracy: 0.7255 - val_loss: 1.8441 - val_accuracy: 0.0588\n",
            "Epoch 91/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1742 - accuracy: 0.7320 - val_loss: 1.8420 - val_accuracy: 0.0588\n",
            "Epoch 92/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1740 - accuracy: 0.7320 - val_loss: 1.8406 - val_accuracy: 0.0588\n",
            "Epoch 93/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1743 - accuracy: 0.7320 - val_loss: 1.8397 - val_accuracy: 0.0588\n",
            "Epoch 94/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1742 - accuracy: 0.7320 - val_loss: 1.8404 - val_accuracy: 0.0588\n",
            "Epoch 95/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1737 - accuracy: 0.7320 - val_loss: 1.8415 - val_accuracy: 0.0588\n",
            "Epoch 96/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1676 - accuracy: 0.7386 - val_loss: 1.8423 - val_accuracy: 0.0588\n",
            "Epoch 97/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1721 - accuracy: 0.7320 - val_loss: 1.7933 - val_accuracy: 0.1176\n",
            "Epoch 98/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2005 - accuracy: 0.7059 - val_loss: 1.7953 - val_accuracy: 0.1176\n",
            "Epoch 99/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1992 - accuracy: 0.7059 - val_loss: 1.7479 - val_accuracy: 0.1765\n",
            "Epoch 100/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1864 - accuracy: 0.7190 - val_loss: 1.7493 - val_accuracy: 0.1765\n",
            "Epoch 101/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1868 - accuracy: 0.7190 - val_loss: 1.7554 - val_accuracy: 0.1176\n",
            "Epoch 102/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1865 - accuracy: 0.7190 - val_loss: 1.7656 - val_accuracy: 0.1176\n",
            "Epoch 103/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1859 - accuracy: 0.7190 - val_loss: 1.7823 - val_accuracy: 0.1176\n",
            "Epoch 104/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1818 - accuracy: 0.7255 - val_loss: 1.7821 - val_accuracy: 0.1176\n",
            "Epoch 105/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1960 - accuracy: 0.7059 - val_loss: 1.8086 - val_accuracy: 0.0588\n",
            "Epoch 106/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2162 - accuracy: 0.6928 - val_loss: 1.3281 - val_accuracy: 0.5882\n",
            "Epoch 107/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1618 - accuracy: 0.7451 - val_loss: 1.2065 - val_accuracy: 0.6471\n",
            "Epoch 108/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2636 - accuracy: 0.6405 - val_loss: 1.7320 - val_accuracy: 0.1765\n",
            "Epoch 109/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1526 - accuracy: 0.7516 - val_loss: 1.7376 - val_accuracy: 0.1765\n",
            "Epoch 110/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1345 - accuracy: 0.7712 - val_loss: 1.7229 - val_accuracy: 0.2353\n",
            "Epoch 111/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0843 - accuracy: 0.8170 - val_loss: 1.7886 - val_accuracy: 0.1176\n",
            "Epoch 112/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0679 - accuracy: 0.8431 - val_loss: 1.7951 - val_accuracy: 0.1176\n",
            "Epoch 113/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0836 - accuracy: 0.8235 - val_loss: 1.7916 - val_accuracy: 0.1176\n",
            "Epoch 114/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0640 - accuracy: 0.8431 - val_loss: 1.7985 - val_accuracy: 0.1176\n",
            "Epoch 115/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0629 - accuracy: 0.8431 - val_loss: 1.7946 - val_accuracy: 0.1176\n",
            "Epoch 116/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0704 - accuracy: 0.8366 - val_loss: 1.8275 - val_accuracy: 0.0588\n",
            "Epoch 117/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.0573 - accuracy: 0.8497 - val_loss: 1.7728 - val_accuracy: 0.1176\n",
            "Epoch 118/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0540 - accuracy: 0.8497 - val_loss: 1.6085 - val_accuracy: 0.2941\n",
            "Epoch 119/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0597 - accuracy: 0.8431 - val_loss: 1.7035 - val_accuracy: 0.1765\n",
            "Epoch 120/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0129 - accuracy: 0.8954 - val_loss: 1.7387 - val_accuracy: 0.1765\n",
            "Epoch 121/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0240 - accuracy: 0.8758 - val_loss: 1.8469 - val_accuracy: 0.0588\n",
            "Epoch 122/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1539 - accuracy: 0.7582 - val_loss: 1.8432 - val_accuracy: 0.0588\n",
            "Epoch 123/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2709 - accuracy: 0.6340 - val_loss: 1.7787 - val_accuracy: 0.1176\n",
            "Epoch 124/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2073 - accuracy: 0.6993 - val_loss: 1.8388 - val_accuracy: 0.0588\n",
            "Epoch 125/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1709 - accuracy: 0.7386 - val_loss: 1.8356 - val_accuracy: 0.0588\n",
            "Epoch 126/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0608 - accuracy: 0.8431 - val_loss: 1.6732 - val_accuracy: 0.2353\n",
            "Epoch 127/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.0912 - accuracy: 0.8105 - val_loss: 1.5479 - val_accuracy: 0.3529\n",
            "Epoch 128/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0978 - accuracy: 0.8105 - val_loss: 1.5516 - val_accuracy: 0.3529\n",
            "Epoch 129/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0875 - accuracy: 0.8170 - val_loss: 1.7876 - val_accuracy: 0.1176\n",
            "Epoch 130/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0622 - accuracy: 0.8431 - val_loss: 1.8447 - val_accuracy: 0.0588\n",
            "Epoch 131/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0601 - accuracy: 0.8431 - val_loss: 1.6758 - val_accuracy: 0.2353\n",
            "Epoch 132/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1934 - accuracy: 0.7124 - val_loss: 1.7743 - val_accuracy: 0.1176\n",
            "Epoch 133/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1279 - accuracy: 0.7778 - val_loss: 1.7273 - val_accuracy: 0.1765\n",
            "Epoch 134/300\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.1176 - accuracy: 0.7843 - val_loss: 1.8080 - val_accuracy: 0.1176\n",
            "Epoch 135/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0995 - accuracy: 0.8039 - val_loss: 1.6489 - val_accuracy: 0.2941\n",
            "Epoch 136/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1072 - accuracy: 0.7974 - val_loss: 1.6742 - val_accuracy: 0.2353\n",
            "Epoch 137/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0603 - accuracy: 0.8431 - val_loss: 1.8405 - val_accuracy: 0.0588\n",
            "Epoch 138/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0450 - accuracy: 0.8562 - val_loss: 1.7891 - val_accuracy: 0.1176\n",
            "Epoch 139/300\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0427 - accuracy: 0.8627 - val_loss: 1.7924 - val_accuracy: 0.1176\n",
            "Epoch 140/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0356 - accuracy: 0.8693 - val_loss: 1.7427 - val_accuracy: 0.1765\n",
            "Epoch 141/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0166 - accuracy: 0.8889 - val_loss: 1.7763 - val_accuracy: 0.1176\n",
            "Epoch 142/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0179 - accuracy: 0.8889 - val_loss: 1.7797 - val_accuracy: 0.1176\n",
            "Epoch 143/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.0221 - accuracy: 0.8824 - val_loss: 1.7887 - val_accuracy: 0.1176\n",
            "Epoch 144/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0197 - accuracy: 0.8889 - val_loss: 1.7762 - val_accuracy: 0.1765\n",
            "Epoch 145/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0161 - accuracy: 0.8889 - val_loss: 1.7566 - val_accuracy: 0.1765\n",
            "Epoch 146/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0134 - accuracy: 0.8889 - val_loss: 1.7940 - val_accuracy: 0.1176\n",
            "Epoch 147/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0101 - accuracy: 0.8954 - val_loss: 1.7605 - val_accuracy: 0.1176\n",
            "Epoch 148/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0111 - accuracy: 0.8954 - val_loss: 1.7861 - val_accuracy: 0.1176\n",
            "Epoch 149/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0035 - accuracy: 0.9020 - val_loss: 1.7352 - val_accuracy: 0.1765\n",
            "Epoch 150/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9828 - accuracy: 0.9150 - val_loss: 1.9011 - val_accuracy: 0.0000e+00\n",
            "Epoch 151/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9905 - accuracy: 0.9150 - val_loss: 1.8982 - val_accuracy: 0.0000e+00\n",
            "Epoch 152/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9906 - accuracy: 0.9150 - val_loss: 1.8853 - val_accuracy: 0.0000e+00\n",
            "Epoch 153/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9906 - accuracy: 0.9150 - val_loss: 1.8664 - val_accuracy: 0.0000e+00\n",
            "Epoch 154/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9906 - accuracy: 0.9150 - val_loss: 1.8722 - val_accuracy: 0.0000e+00\n",
            "Epoch 155/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9907 - accuracy: 0.9150 - val_loss: 1.8635 - val_accuracy: 0.0588\n",
            "Epoch 156/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9908 - accuracy: 0.9150 - val_loss: 1.8616 - val_accuracy: 0.0588\n",
            "Epoch 157/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9908 - accuracy: 0.9150 - val_loss: 1.8566 - val_accuracy: 0.0588\n",
            "Epoch 158/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9905 - accuracy: 0.9150 - val_loss: 1.8565 - val_accuracy: 0.0588\n",
            "Epoch 159/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9912 - accuracy: 0.9150 - val_loss: 1.8676 - val_accuracy: 0.0588\n",
            "Epoch 160/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9904 - accuracy: 0.9150 - val_loss: 1.8702 - val_accuracy: 0.0588\n",
            "Epoch 161/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9904 - accuracy: 0.9150 - val_loss: 1.8670 - val_accuracy: 0.0588\n",
            "Epoch 162/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9904 - accuracy: 0.9150 - val_loss: 1.8672 - val_accuracy: 0.0000e+00\n",
            "Epoch 163/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9840 - accuracy: 0.9216 - val_loss: 1.8722 - val_accuracy: 0.0000e+00\n",
            "Epoch 164/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9836 - accuracy: 0.9216 - val_loss: 1.8747 - val_accuracy: 0.0000e+00\n",
            "Epoch 165/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9838 - accuracy: 0.9216 - val_loss: 1.8755 - val_accuracy: 0.0000e+00\n",
            "Epoch 166/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9836 - accuracy: 0.9216 - val_loss: 1.8756 - val_accuracy: 0.0000e+00\n",
            "Epoch 167/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9843 - accuracy: 0.9216 - val_loss: 1.8768 - val_accuracy: 0.0000e+00\n",
            "Epoch 168/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9839 - accuracy: 0.9216 - val_loss: 1.8788 - val_accuracy: 0.0000e+00\n",
            "Epoch 169/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9836 - accuracy: 0.9216 - val_loss: 1.8774 - val_accuracy: 0.0000e+00\n",
            "Epoch 170/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9838 - accuracy: 0.9216 - val_loss: 1.8779 - val_accuracy: 0.0000e+00\n",
            "Epoch 171/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9839 - accuracy: 0.9216 - val_loss: 1.8779 - val_accuracy: 0.0000e+00\n",
            "Epoch 172/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9839 - accuracy: 0.9216 - val_loss: 1.8751 - val_accuracy: 0.0000e+00\n",
            "Epoch 173/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9838 - accuracy: 0.9216 - val_loss: 1.8719 - val_accuracy: 0.0000e+00\n",
            "Epoch 174/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9841 - accuracy: 0.9216 - val_loss: 1.8688 - val_accuracy: 0.0588\n",
            "Epoch 175/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9837 - accuracy: 0.9216 - val_loss: 1.8669 - val_accuracy: 0.0588\n",
            "Epoch 176/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9839 - accuracy: 0.9216 - val_loss: 1.8665 - val_accuracy: 0.0588\n",
            "Epoch 177/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9838 - accuracy: 0.9216 - val_loss: 1.8659 - val_accuracy: 0.0588\n",
            "Epoch 178/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9834 - accuracy: 0.9216 - val_loss: 1.8642 - val_accuracy: 0.0588\n",
            "Epoch 179/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9769 - accuracy: 0.9281 - val_loss: 1.7738 - val_accuracy: 0.1176\n",
            "Epoch 180/300\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9975 - accuracy: 0.9085 - val_loss: 1.7548 - val_accuracy: 0.1765\n",
            "Epoch 181/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1748 - accuracy: 0.7255 - val_loss: 1.8396 - val_accuracy: 0.0588\n",
            "Epoch 182/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1476 - accuracy: 0.7582 - val_loss: 1.7644 - val_accuracy: 0.1176\n",
            "Epoch 183/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1337 - accuracy: 0.7647 - val_loss: 1.6749 - val_accuracy: 0.2353\n",
            "Epoch 184/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3721 - accuracy: 0.5359 - val_loss: 1.8500 - val_accuracy: 0.0588\n",
            "Epoch 185/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3373 - accuracy: 0.5621 - val_loss: 1.8464 - val_accuracy: 0.0588\n",
            "Epoch 186/300\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.3714 - accuracy: 0.5294 - val_loss: 1.7313 - val_accuracy: 0.1765\n",
            "Epoch 187/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3380 - accuracy: 0.5621 - val_loss: 1.9025 - val_accuracy: 0.0000e+00\n",
            "Epoch 188/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2604 - accuracy: 0.6471 - val_loss: 1.8456 - val_accuracy: 0.0588\n",
            "Epoch 189/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2732 - accuracy: 0.6275 - val_loss: 1.8763 - val_accuracy: 0.0588\n",
            "Epoch 190/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2910 - accuracy: 0.6144 - val_loss: 1.8457 - val_accuracy: 0.0588\n",
            "Epoch 191/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2426 - accuracy: 0.6536 - val_loss: 1.8675 - val_accuracy: 0.0588\n",
            "Epoch 192/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.2638 - accuracy: 0.6405 - val_loss: 1.8721 - val_accuracy: 0.0000e+00\n",
            "Epoch 193/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2630 - accuracy: 0.6405 - val_loss: 1.9047 - val_accuracy: 0.0000e+00\n",
            "Epoch 194/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2908 - accuracy: 0.6144 - val_loss: 1.9045 - val_accuracy: 0.0000e+00\n",
            "Epoch 195/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2954 - accuracy: 0.6078 - val_loss: 1.9047 - val_accuracy: 0.0000e+00\n",
            "Epoch 196/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4311 - accuracy: 0.4706 - val_loss: 1.9006 - val_accuracy: 0.0000e+00\n",
            "Epoch 197/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.3944 - accuracy: 0.5098 - val_loss: 1.9002 - val_accuracy: 0.0000e+00\n",
            "Epoch 198/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.2535 - accuracy: 0.6471 - val_loss: 1.7951 - val_accuracy: 0.1176\n",
            "Epoch 199/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2357 - accuracy: 0.6732 - val_loss: 1.8451 - val_accuracy: 0.0588\n",
            "Epoch 200/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2584 - accuracy: 0.6405 - val_loss: 1.8459 - val_accuracy: 0.0588\n",
            "Epoch 201/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2286 - accuracy: 0.6797 - val_loss: 1.8341 - val_accuracy: 0.0588\n",
            "Epoch 202/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2193 - accuracy: 0.6863 - val_loss: 1.7287 - val_accuracy: 0.1765\n",
            "Epoch 203/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 1.2179 - accuracy: 0.6863 - val_loss: 1.6879 - val_accuracy: 0.2353\n",
            "Epoch 204/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.2188 - accuracy: 0.6863 - val_loss: 1.7284 - val_accuracy: 0.1765\n",
            "Epoch 205/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2183 - accuracy: 0.6863 - val_loss: 1.7284 - val_accuracy: 0.1765\n",
            "Epoch 206/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2186 - accuracy: 0.6863 - val_loss: 1.7284 - val_accuracy: 0.1765\n",
            "Epoch 207/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2181 - accuracy: 0.6863 - val_loss: 1.7905 - val_accuracy: 0.1176\n",
            "Epoch 208/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2206 - accuracy: 0.6863 - val_loss: 1.7310 - val_accuracy: 0.1765\n",
            "Epoch 209/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2183 - accuracy: 0.6863 - val_loss: 1.6184 - val_accuracy: 0.2941\n",
            "Epoch 210/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2121 - accuracy: 0.6928 - val_loss: 1.7204 - val_accuracy: 0.1765\n",
            "Epoch 211/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2117 - accuracy: 0.6928 - val_loss: 1.7238 - val_accuracy: 0.1765\n",
            "Epoch 212/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2111 - accuracy: 0.6928 - val_loss: 1.7083 - val_accuracy: 0.1765\n",
            "Epoch 213/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2098 - accuracy: 0.6928 - val_loss: 1.7038 - val_accuracy: 0.1765\n",
            "Epoch 214/300\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.2114 - accuracy: 0.6928 - val_loss: 1.7156 - val_accuracy: 0.1765\n",
            "Epoch 215/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2110 - accuracy: 0.6928 - val_loss: 1.6093 - val_accuracy: 0.2941\n",
            "Epoch 216/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.2048 - accuracy: 0.6993 - val_loss: 1.5791 - val_accuracy: 0.3529\n",
            "Epoch 217/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2224 - accuracy: 0.6797 - val_loss: 1.7369 - val_accuracy: 0.1765\n",
            "Epoch 218/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2119 - accuracy: 0.6928 - val_loss: 1.7286 - val_accuracy: 0.1765\n",
            "Epoch 219/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2112 - accuracy: 0.6928 - val_loss: 1.9037 - val_accuracy: 0.0000e+00\n",
            "Epoch 220/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2406 - accuracy: 0.6667 - val_loss: 1.7304 - val_accuracy: 0.1765\n",
            "Epoch 221/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2475 - accuracy: 0.6536 - val_loss: 1.6809 - val_accuracy: 0.2353\n",
            "Epoch 222/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2157 - accuracy: 0.6863 - val_loss: 1.7734 - val_accuracy: 0.1176\n",
            "Epoch 223/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2112 - accuracy: 0.6928 - val_loss: 1.7422 - val_accuracy: 0.1765\n",
            "Epoch 224/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2104 - accuracy: 0.6928 - val_loss: 1.7359 - val_accuracy: 0.1765\n",
            "Epoch 225/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2114 - accuracy: 0.6928 - val_loss: 1.7458 - val_accuracy: 0.1765\n",
            "Epoch 226/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2100 - accuracy: 0.6928 - val_loss: 1.7412 - val_accuracy: 0.1765\n",
            "Epoch 227/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.2106 - accuracy: 0.6928 - val_loss: 1.7580 - val_accuracy: 0.1176\n",
            "Epoch 228/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2095 - accuracy: 0.6928 - val_loss: 1.7701 - val_accuracy: 0.1176\n",
            "Epoch 229/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.2130 - accuracy: 0.6928 - val_loss: 1.7357 - val_accuracy: 0.1765\n",
            "Epoch 230/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2109 - accuracy: 0.6863 - val_loss: 1.7414 - val_accuracy: 0.1765\n",
            "Epoch 231/300\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.2080 - accuracy: 0.6928 - val_loss: 1.6525 - val_accuracy: 0.2353\n",
            "Epoch 232/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2075 - accuracy: 0.6928 - val_loss: 1.6329 - val_accuracy: 0.2353\n",
            "Epoch 233/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2064 - accuracy: 0.6928 - val_loss: 1.6526 - val_accuracy: 0.2353\n",
            "Epoch 234/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2088 - accuracy: 0.6928 - val_loss: 1.6284 - val_accuracy: 0.2353\n",
            "Epoch 235/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2022 - accuracy: 0.6993 - val_loss: 1.6679 - val_accuracy: 0.2353\n",
            "Epoch 236/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2000 - accuracy: 0.6993 - val_loss: 1.6718 - val_accuracy: 0.2353\n",
            "Epoch 237/300\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.1696 - accuracy: 0.7320 - val_loss: 1.6577 - val_accuracy: 0.2353\n",
            "Epoch 238/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1473 - accuracy: 0.7582 - val_loss: 1.5676 - val_accuracy: 0.3529\n",
            "Epoch 239/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1232 - accuracy: 0.7712 - val_loss: 1.8625 - val_accuracy: 0.0000e+00\n",
            "Epoch 240/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.0966 - accuracy: 0.8105 - val_loss: 1.8293 - val_accuracy: 0.0588\n",
            "Epoch 241/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0478 - accuracy: 0.8562 - val_loss: 1.8225 - val_accuracy: 0.0588\n",
            "Epoch 242/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.0380 - accuracy: 0.8627 - val_loss: 1.6553 - val_accuracy: 0.2353\n",
            "Epoch 243/300\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.0384 - accuracy: 0.8562 - val_loss: 1.8410 - val_accuracy: 0.0588\n",
            "Epoch 244/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1639 - accuracy: 0.7320 - val_loss: 1.6635 - val_accuracy: 0.2353\n",
            "Epoch 245/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9988 - accuracy: 0.9085 - val_loss: 1.5777 - val_accuracy: 0.3529\n",
            "Epoch 246/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0044 - accuracy: 0.9020 - val_loss: 1.5528 - val_accuracy: 0.3529\n",
            "Epoch 247/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0004 - accuracy: 0.9020 - val_loss: 1.6376 - val_accuracy: 0.2353\n",
            "Epoch 248/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1839 - accuracy: 0.7124 - val_loss: 1.5776 - val_accuracy: 0.2941\n",
            "Epoch 249/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1536 - accuracy: 0.7582 - val_loss: 1.5261 - val_accuracy: 0.3529\n",
            "Epoch 250/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1702 - accuracy: 0.7451 - val_loss: 1.6753 - val_accuracy: 0.2353\n",
            "Epoch 251/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1429 - accuracy: 0.7647 - val_loss: 1.5641 - val_accuracy: 0.3529\n",
            "Epoch 252/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0434 - accuracy: 0.8627 - val_loss: 1.5804 - val_accuracy: 0.2941\n",
            "Epoch 253/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0283 - accuracy: 0.8758 - val_loss: 1.6343 - val_accuracy: 0.2353\n",
            "Epoch 254/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9916 - accuracy: 0.9150 - val_loss: 1.7047 - val_accuracy: 0.1765\n",
            "Epoch 255/300\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9785 - accuracy: 0.9281 - val_loss: 1.6937 - val_accuracy: 0.2353\n",
            "Epoch 256/300\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9755 - accuracy: 0.9281 - val_loss: 1.7218 - val_accuracy: 0.1765\n",
            "Epoch 257/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9710 - accuracy: 0.9346 - val_loss: 1.7242 - val_accuracy: 0.1765\n",
            "Epoch 258/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9710 - accuracy: 0.9346 - val_loss: 1.7245 - val_accuracy: 0.1765\n",
            "Epoch 259/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9711 - accuracy: 0.9346 - val_loss: 1.7247 - val_accuracy: 0.1765\n",
            "Epoch 260/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9709 - accuracy: 0.9346 - val_loss: 1.7249 - val_accuracy: 0.1765\n",
            "Epoch 261/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9707 - accuracy: 0.9346 - val_loss: 1.7251 - val_accuracy: 0.1765\n",
            "Epoch 262/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9707 - accuracy: 0.9346 - val_loss: 1.7255 - val_accuracy: 0.1765\n",
            "Epoch 263/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9710 - accuracy: 0.9346 - val_loss: 1.7256 - val_accuracy: 0.1765\n",
            "Epoch 264/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7256 - val_accuracy: 0.1765\n",
            "Epoch 265/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9707 - accuracy: 0.9346 - val_loss: 1.7252 - val_accuracy: 0.1765\n",
            "Epoch 266/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7247 - val_accuracy: 0.1765\n",
            "Epoch 267/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7246 - val_accuracy: 0.1765\n",
            "Epoch 268/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9707 - accuracy: 0.9346 - val_loss: 1.7245 - val_accuracy: 0.1765\n",
            "Epoch 269/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9705 - accuracy: 0.9346 - val_loss: 1.7246 - val_accuracy: 0.1765\n",
            "Epoch 270/300\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9700 - accuracy: 0.9346 - val_loss: 1.7245 - val_accuracy: 0.1765\n",
            "Epoch 271/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9700 - accuracy: 0.9346 - val_loss: 1.7247 - val_accuracy: 0.1765\n",
            "Epoch 272/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9695 - accuracy: 0.9346 - val_loss: 1.7245 - val_accuracy: 0.1765\n",
            "Epoch 273/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7245 - val_accuracy: 0.1765\n",
            "Epoch 274/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7246 - val_accuracy: 0.1765\n",
            "Epoch 275/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9703 - accuracy: 0.9346 - val_loss: 1.7245 - val_accuracy: 0.1765\n",
            "Epoch 276/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7243 - val_accuracy: 0.1765\n",
            "Epoch 277/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7242 - val_accuracy: 0.1765\n",
            "Epoch 278/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9703 - accuracy: 0.9346 - val_loss: 1.7237 - val_accuracy: 0.1765\n",
            "Epoch 279/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7232 - val_accuracy: 0.1765\n",
            "Epoch 280/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7233 - val_accuracy: 0.1765\n",
            "Epoch 281/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9705 - accuracy: 0.9346 - val_loss: 1.7232 - val_accuracy: 0.1765\n",
            "Epoch 282/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7232 - val_accuracy: 0.1765\n",
            "Epoch 283/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9692 - accuracy: 0.9346 - val_loss: 1.7234 - val_accuracy: 0.1765\n",
            "Epoch 284/300\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7233 - val_accuracy: 0.1765\n",
            "Epoch 285/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7231 - val_accuracy: 0.1765\n",
            "Epoch 286/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9700 - accuracy: 0.9346 - val_loss: 1.7231 - val_accuracy: 0.1765\n",
            "Epoch 287/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9702 - accuracy: 0.9346 - val_loss: 1.7237 - val_accuracy: 0.1765\n",
            "Epoch 288/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7235 - val_accuracy: 0.1765\n",
            "Epoch 289/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7234 - val_accuracy: 0.1765\n",
            "Epoch 290/300\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7229 - val_accuracy: 0.1765\n",
            "Epoch 291/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7227 - val_accuracy: 0.1765\n",
            "Epoch 292/300\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7226 - val_accuracy: 0.1765\n",
            "Epoch 293/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9696 - accuracy: 0.9346 - val_loss: 1.7228 - val_accuracy: 0.1765\n",
            "Epoch 294/300\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7229 - val_accuracy: 0.1765\n",
            "Epoch 295/300\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9705 - accuracy: 0.9346 - val_loss: 1.7228 - val_accuracy: 0.1765\n",
            "Epoch 296/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7227 - val_accuracy: 0.1765\n",
            "Epoch 297/300\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7226 - val_accuracy: 0.1765\n",
            "Epoch 298/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9704 - accuracy: 0.9346 - val_loss: 1.7223 - val_accuracy: 0.1765\n",
            "Epoch 299/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9705 - accuracy: 0.9346 - val_loss: 1.7220 - val_accuracy: 0.1765\n",
            "Epoch 300/300\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9706 - accuracy: 0.9346 - val_loss: 1.7218 - val_accuracy: 0.1765\n",
            "timeToTrain = 26.804697036743164 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer,batch_size=\"Nadam\",64\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print(\"Test accuracy = \", acc)\n",
        "\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "# axs[0].plot(range(1,len(history.history['accuracy'])+1),history.history['accuracy'])\n",
        "# axs[0].set_title(f'Acc - {lossFxn[:3]},{batch_size},{optimizer}')\n",
        "# axs[1].plot(range(1,len(history.history['loss'])+1),history.history['loss'],'tab:red')\n",
        "# axs[1].set_title(f'Loss - {lossFxn[:3]},{batch_size},{optimizer}')\n",
        "\n",
        "axs[0, 0].plot(range(1,len(history.history['accuracy'])+1),history.history['accuracy'])\n",
        "axs[0, 0].set_title(f'Train Acc - {batch_size},{optimizer}')\n",
        "axs[1, 0].plot(range(1,len(history.history['loss'])+1),history.history['loss'],'tab:red')\n",
        "axs[1, 0].set_title(f'Train Loss - {batch_size},{optimizer}')\n",
        "axs[0, 1].plot(range(1,len(history.history['val_accuracy'])+1),history.history['val_accuracy'])\n",
        "axs[0, 1].set_title(f'Val Acc - {batch_size},{optimizer}')\n",
        "axs[1, 1].plot(range(1,len(history.history['val_loss'])+1),history.history['val_loss'],'tab:red')\n",
        "axs[1, 1].set_title(f'Val Loss - {batch_size},{optimizer}')\n",
        "\n",
        "'''axs[0, 0].plot(history.history['accuracy'])\n",
        "axs[0, 0].set_title('Train dataset Accuracy')\n",
        "axs[0, 1].plot(history.history['loss'])\n",
        "axs[0, 1].set_title('Training Loss')\n",
        "axs[1, 0].plot(history.history['val_accuracy'])\n",
        "axs[1, 0].set_title('Validation dataset Accuracy')\n",
        "axs[1, 1].plot(history.history['val_loss'])\n",
        "axs[1, 1].set_title('Validation Loss')'''\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel='Epochs', ylabel='Parameter')\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "# '''for ax in axs.flat:\n",
        "#     ax.label_outer()'''\n",
        "plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.5,hspace=0.5)\n",
        "# plt.savefig(f\"/content/drive/MyDrive/Emotional_AI_Chatbot/plots3/Row{j}.png\")\n",
        "j+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "-gPppaJ4yvxi",
        "outputId": "4b7999aa-7cde-4bc8-8be0-9bf56ad851f4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2041 - accuracy: 0.7000\n",
            "Test accuracy =  0.699999988079071\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emoji key:\n",
        "# ❤️ = Angry\n",
        "# ⚾ = Anxious\n",
        "# 😄 = Confident\n",
        "# 😞 = Happy\n",
        "# 🍴 = Sad\n",
        "\n",
        "def label_to_emotion(label):\n",
        "  if label == 0:\n",
        "    return \"--> Angry\"\n",
        "  elif label == 1:\n",
        "    return \"--> Anxious\"\n",
        "  elif label == 2:\n",
        "    return \"--> Confident\"\n",
        "  elif label == 3:\n",
        "    return \"--> Happy\"\n",
        "  else:\n",
        "    return \"--> Sad\""
      ],
      "metadata": {
        "id": "R24fQQ6yRGbK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "phrases=[\"Why are you so bad\", \"I am concerned\", \"We will succeed\", \"That was a really good movie\", \"My girlfriend dumped me\"]\n",
        "phrase = phrases[0]\n",
        "x_test = np.array([phrase])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "output = np.argmax(model.predict(X_test_indices))\n",
        "\n",
        "print(x_test[0] + ' ' + label_to_emotion(output))\n",
        "\n",
        "# Keep in mind that sentences can correspond to multiple emotion classes - even if it doesn't match what you originally thought, it might make sense."
      ],
      "metadata": {
        "id": "7uzxKBoy0nul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
